To Do List
============================
1. Run Cartpole with openai DQN --> DONE
    - Simple observation and action space
    - Not image based
    - Result: It works! Only took ~20 min
    * Warning: apparently set_global_seeds causes the unstable training?
2. Run Pong with openai DQN --> Stage B
    - Still simple action space
    - Image based --> Requires CNN and atari wrappers
    - Result: It works! Took ~6 hours (only got ~19.8 reward though?)
    = Look into train.py script inside atari folder
    = Still need to test with different wrappers enabled
    = How does changing the FC layers from 256 --> 512 affect training?
    = Buffer size: most others use 100K or 1M (need a lot of ram)
3. Run Breakout with openai DQN --> Stage B
    - Known to converge slower
    - Result: Only got to 16.0 reward with 2M steps
    = Run for 10M steps with GTX 1080
    = Try with 100K replay buffer then 512 FC layers
4a. Run pygame pong with native DQN implementation
    - Are there are issues with pygame implementations?
    = It works! Not issue with pygame
4b. Run pygame pong with OpenAI DQN
    - Made pygame version of pong based on PLE
    - Are there issues with integrating pygame with OpenAI DQN?
    = It works! But it seems to train slower than the gym pong?
5. Run cytomatrix with OpenAI DQN
    = It works!
    = The problem was due mainly to exploration; increase final eps to 0.1
    = Need to find an optimal epsilon schedule


Create the Final Training Script
================================
1. Run custom_cartpole.py
2. Compare custom_cartpole.py and custom_pong.py
3. Modify dqn.py based on #2
